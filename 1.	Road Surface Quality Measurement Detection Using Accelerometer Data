Jupyter Notebook
latest capstone file
(autosaved)
Current Kernel Logo
Python 3 
File
Edit
View
Insert
Cell
Kernel
Widgets
Help

Problem Statement
Prediction of Road Quality using Decision Tree based methods
In this project, we predict the quality of Roads. We classify the Roads mainly in three types based on their existing condition and structure. Type-1 Carpet Roads Type-2 Bumpy Roads Type-3 Rough Roads

For the data collection of this project, I collected it myself. I mounted a smart phone in a Car and recorded the data with the help of a simple web application

which can be easily, the results were recorded... After the collection of data, Feature preprocessing techniques were employed including Taking the fourier transform of the data. I cleaned the data with the help of a Low pass Filter which significantly increased the prediction power of the classifier For model selction, I tried different models including LSTM, CNN, Simple Feed forward neural networks and Decision Tree based methods such as Gradient Boosting Classifier and Random Forest Classifier... The Gradient Boosting Trees Based classifier yielded the greatest accuracy as I will demonstrate in this notebook

Adding the data via Cloudant DB
Following is the file for the Carpet Roads and since this is a supervised learning method, we simply Label them as '1' i.e the positive case. The data consists of 6 colunmns. First three columns corresponds to the values of the accelerometer readings namely X-axis, Y-axis and Z-axis readinds The second columns corresponds to the readings of the gyroscope amely Xg-axis, Yg-axis and Zg-axis readinds


import sys
import types
import pandas as pd

df_data_1 = pd.read_csv('train.csv')
df_data_1.head()

X	Y	Z	Xg	Yg	Zg	Label
0	-0.076614	2.164358	8.734048	0.002758	-0.039410	-0.024923	1
1	0.076614	0.057461	7.354988	-0.003246	-0.018500	-0.001449	1
2	-0.497994	3.160346	9.806650	0.005149	-0.008971	0.017383	1
3	-0.306458	1.398214	10.879252	0.001937	-0.005288	0.018309	1
4	-0.191536	2.738967	9.040505	-0.018832	-0.003665	0.017506	1

df_combined_carpet_road_data=df_data_1

​
import pip
​
try:
    __import__('keras')
except ImportError:
    pip.main(['install', 'keras']) 
​
Using TensorFlow backend.

​
import numpy as np
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
import sklearn
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import sys
from queue import Queue
import pandas as pd
import json

df_combined_carpet_road_data.head(10)
X	Y	Z	Xg	Yg	Zg	Label
0	-0.076614	2.164358	8.734048	0.002758	-0.039410	-0.024923	1
1	0.076614	0.057461	7.354988	-0.003246	-0.018500	-0.001449	1
2	-0.497994	3.160346	9.806650	0.005149	-0.008971	0.017383	1
3	-0.306458	1.398214	10.879252	0.001937	-0.005288	0.018309	1
4	-0.191536	2.738967	9.040505	-0.018832	-0.003665	0.017506	1
5	-0.881066	3.428497	9.768343	-0.041399	0.001449	0.022916	1
6	-0.268151	2.087744	9.461885	-0.029740	0.002391	0.009913	1
7	-0.804452	4.386178	10.572795	-0.017314	0.007435	0.000262	1
8	-0.957681	2.011129	8.274361	-0.020298	0.026878	-0.006824	1
9	-0.268151	2.853889	8.006210	0.012793	0.048991	-0.016773	1

df_combined_carpet_road_data.tail(10)
X	Y	Z	Xg	Yg	Zg	Label
11496	0.076614	1.972822	10.151415	0.005166	0.044628	0.148388	1
11497	-0.306458	2.279280	9.921572	0.019321	0.045169	0.132313	1
11498	-0.344765	1.589750	8.274361	0.003543	0.042237	0.117181	1
11499	-0.421380	2.624045	7.431602	0.004328	0.030665	0.083671	1
11500	-0.229843	3.007117	9.232041	0.011502	0.023719	0.054978	1
11501	-0.268151	2.624045	9.538500	0.014207	0.019635	0.039776	1
11502	-0.153229	1.168370	8.121132	0.009809	0.018780	0.022689	1
11503	-0.306458	2.087744	9.844957	0.013230	0.021625	0.017907	1
11504	0.229843	1.819593	9.385271	0.010891	0.020996	0.028117	1
11505	-0.114922	2.164358	9.040505	0.020769	0.016825	0.022148	1

df_combined_carpet_road_data.describe()
X	Y	Z	Xg	Yg	Zg	Label
count	11506.000000	11506.000000	11506.000000	11506.000000	11506.000000	11506.000000	11506.0
mean	-0.343886	2.393369	9.242815	-0.000259	0.000096	0.000455	1.0
std	0.293409	1.005678	1.121789	0.064532	0.037413	0.063992	0.0
min	-1.647211	-0.823605	4.213795	-0.267856	-0.167255	-0.243351	1.0
25%	-0.536301	1.704672	8.465898	-0.038039	-0.022393	-0.039475	1.0
50%	-0.344765	2.279280	9.270349	-0.000175	0.000524	0.000140	1.0
75%	-0.153229	3.083732	10.036493	0.037813	0.023143	0.040801	1.0
max	1.302446	6.722918	14.709975	0.291156	0.160797	0.279427	1.0
calculating the Correlation matrix between different columns of the data

np.corrcoef(df_combined_carpet_road_data['X'], df_combined_carpet_road_data['Y'])
array([[ 1.        , -0.16336341],
       [-0.16336341,  1.        ]])

np.corrcoef(df_combined_carpet_road_data['X'], df_combined_carpet_road_data['Z'])
array([[ 1.        , -0.06797856],
       [-0.06797856,  1.        ]])

np.corrcoef(df_combined_carpet_road_data['Y'], df_combined_carpet_road_data['Yg'])
array([[ 1.        , -0.01258369],
       [-0.01258369,  1.        ]])
The columns are very little correlated i.e they are assumed to be affecting little on the final result

df_combined_carpet_road_data.info(10)
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 11506 entries, 0 to 11505
Data columns (total 7 columns):
X        11506 non-null float64
Y        11506 non-null float64
Z        11506 non-null float64
Xg       11506 non-null float64
Yg       11506 non-null float64
Zg       11506 non-null float64
Label    11506 non-null int64
dtypes: float64(6), int64(1)
memory usage: 629.3 KB

df_combined_carpet_road_data.shape
(11506, 7)
plotting the data ( we plot all the 6 axis of our readings )
Three of them comes from the accelerometer and three of them comes from gyroscope reading
Now , we will plot our data and see what it looks like


plt.hist(df_combined_carpet_road_data['X'])
#df_combined_carpet_road_data.X.plot()
(array([  9.00000000e+00,   1.32000000e+02,   8.47000000e+02,
          2.54700000e+03,   4.80000000e+03,   2.63600000e+03,
          4.83000000e+02,   5.10000000e+01,   0.00000000e+00,
          1.00000000e+00]),
 array([-1.6472107 , -1.35224505, -1.0572794 , -0.76231375, -0.4673481 ,
        -0.17238245,  0.1225832 ,  0.41754885,  0.7125145 ,  1.00748015,
         1.3024458 ]),
 <a list of 10 Patch objects>)


plt.hist(df_combined_carpet_road_data['Y'])
#df_combined_carpet_road_data.Y.plot()
(array([   38.,   338.,  1526.,  3351.,  3022.,  1970.,   957.,   245.,
           48.,    11.]),
 array([-0.82360536, -0.06895297,  0.68569941,  1.4403518 ,  2.19500418,
         2.94965657,  3.70430896,  4.45896134,  5.21361373,  5.96826611,
         6.7229185 ]),
 <a list of 10 Patch objects>)


plt.hist(df_combined_carpet_road_data['Z'])
(array([    7.,    43.,   521.,  2142.,  3691.,  3637.,  1292.,   162.,
            7.,     4.]),
 array([  4.213795,   5.263413,   6.313031,   7.362649,   8.412267,
          9.461885,  10.511503,  11.561121,  12.610739,  13.660357,
         14.709975]),
 <a list of 10 Patch objects>)


plt.hist(df_combined_carpet_road_data['Xg'])
(array([   20.,   124.,   537.,  1849.,  4211.,  3271.,  1122.,   282.,
           79.,    11.]),
 array([-0.26785567, -0.21195452, -0.15605337, -0.10015222, -0.04425107,
         0.01165008,  0.06755122,  0.12345237,  0.17935352,  0.23525467,
         0.29115582]),
 <a list of 10 Patch objects>)


plt.hist(df_combined_carpet_road_data['Yg'])
(array([    5.,    67.,   386.,  1289.,  3428.,  4055.,  1748.,   417.,
           96.,    15.]),
 array([-0.1672549 , -0.13444969, -0.10164448, -0.06883928, -0.03603407,
        -0.00322886,  0.02957635,  0.06238156,  0.09518676,  0.12799197,
         0.16079718]),
 <a list of 10 Patch objects>)


plt.hist(df_combined_carpet_road_data['Zg'])
(array([   40.,   203.,   665.,  2312.,  3884.,  2941.,  1118.,   292.,
           39.,    12.]),
 array([-0.24335125, -0.1910734 , -0.13879556, -0.08651771, -0.03423987,
         0.01803798,  0.07031582,  0.12259367,  0.17487151,  0.22714936,
         0.2794272 ]),
 <a list of 10 Patch objects>)

Type Markdown and LaTeX: α2α2
Adding the Negative Examples ( The data of Bumpy and Rough Roads)

df_data_2 = pd.read_csv('test.csv')
df_data_2.head()
​
​
X	Y	Z	Xg	Yg	Zg	Label
0	0.306458	2.011129	6.512228	-0.007941	-0.167447	0.034366	0
1	0.651223	2.355895	9.078813	-0.032830	-0.261572	0.024906	0
2	0.497994	1.781286	10.189722	-0.018570	-0.316550	0.006877	0
3	-0.038307	0.746991	9.921572	-0.041207	-0.321018	-0.001326	0
4	0.114922	1.819593	8.121132	-0.129469	-0.299027	-0.024103	0

df_combined_bumpy_road_data=df_data_2

df_combined_bumpy_road_data.head(10)
X	Y	Z	Xg	Yg	Zg	Label
0	0.306458	2.011129	6.512228	-0.007941	-0.167447	0.034366	0
1	0.651223	2.355895	9.078813	-0.032830	-0.261572	0.024906	0
2	0.497994	1.781286	10.189722	-0.018570	-0.316550	0.006877	0
3	-0.038307	0.746991	9.921572	-0.041207	-0.321018	-0.001326	0
4	0.114922	1.819593	8.121132	-0.129469	-0.299027	-0.024103	0
5	0.459687	3.849876	7.738060	-0.125350	-0.296828	-0.024138	0
6	-0.344765	2.853889	7.738060	-0.077196	-0.308120	-0.010891	0
7	-0.344765	2.700659	8.197746	-0.039130	-0.266407	-0.015307	0
8	0.000000	4.462792	7.891289	0.093026	-0.240192	-0.019844	0
9	-0.268151	3.236961	9.730036	0.179978	-0.213384	-0.035256	0

df_combined_bumpy_road_data.tail(10)
X	Y	Z	Xg	Yg	Zg	Label
9204	-0.344765	4.539406	7.469909	-0.030247	-0.011851	-0.103655	0
9205	0.076614	2.087744	8.657433	-0.026040	-0.010140	-0.096936	0
9206	-0.306458	2.738967	6.971915	0.050684	-0.016336	-0.083706	0
9207	0.000000	1.283292	6.818687	0.121091	-0.034784	-0.057229	0
9208	-0.995988	1.015142	8.006210	0.130778	-0.063181	-0.039689	0
9209	-0.038307	0.517148	15.284584	0.059132	-0.062064	-0.034819	0
9210	-0.421380	2.470816	9.959879	0.043092	-0.035989	-0.031940	0
9211	0.191536	1.091756	10.036493	0.010018	-0.029863	-0.044104	0
9212	-0.076614	2.738967	9.346964	-0.011240	-0.034558	-0.061872	0
9213	0.153229	0.823605	5.018247	0.019984	-0.028798	-0.054716	0

df_combined_bumpy_road_data.describe()
X	Y	Z	Xg	Yg	Zg	Label
count	9214.000000	9214.000000	9214.000000	9214.000000	9214.000000	9214.000000	9214.0
mean	-0.331519	2.376786	9.208706	-0.001900	0.002019	0.007706	0.0
std	0.633370	1.833260	2.283853	0.135962	0.075565	0.108221	0.0
min	-5.248090	-6.071695	-2.681506	-0.792641	-0.697695	-0.389383	0.0
25%	-0.689530	1.091756	7.699753	-0.078330	-0.038655	-0.059071	0.0
50%	-0.306458	2.279280	9.212888	-0.001937	0.001073	0.000227	0.0
75%	0.038307	3.543418	10.687716	0.072222	0.042150	0.064001	0.0
max	11.147403	10.017340	20.685904	1.067391	0.643154	0.903522	0.0

df_combined_bumpy_road_data.describe()
X	Y	Z	Xg	Yg	Zg	Label
count	9214.000000	9214.000000	9214.000000	9214.000000	9214.000000	9214.000000	9214.0
mean	-0.331519	2.376786	9.208706	-0.001900	0.002019	0.007706	0.0
std	0.633370	1.833260	2.283853	0.135962	0.075565	0.108221	0.0
min	-5.248090	-6.071695	-2.681506	-0.792641	-0.697695	-0.389383	0.0
25%	-0.689530	1.091756	7.699753	-0.078330	-0.038655	-0.059071	0.0
50%	-0.306458	2.279280	9.212888	-0.001937	0.001073	0.000227	0.0
75%	0.038307	3.543418	10.687716	0.072222	0.042150	0.064001	0.0
max	11.147403	10.017340	20.685904	1.067391	0.643154	0.903522	0.0
Visualizing the data belonging to the negative instance

#df_combined_bumpy_road_data.X.plot()
plt.hist(df_combined_bumpy_road_data['Z'])
(array([  3.00000000e+00,   1.30000000e+01,   1.48000000e+02,
          1.01200000e+03,   3.14200000e+03,   3.34600000e+03,
          1.29400000e+03,   2.34000000e+02,   2.00000000e+01,
          2.00000000e+00]),
 array([ -2.681506,  -0.344765,   1.991976,   4.328717,   6.665458,
          9.002199,  11.33894 ,  13.675681,  16.012422,  18.349163,
         20.685904]),
 <a list of 10 Patch objects>)


#df_combined_bumpy_road_data.Y.plot()
plt.hist(df_combined_bumpy_road_data['Y'])
(array([    3.,     9.,   143.,  1037.,  2853.,  2935.,  1545.,   583.,
           98.,     8.]),
 array([ -6.0716953 ,  -4.46279177,  -2.85388824,  -1.24498471,
          0.36391882,   1.97282235,   3.58172588,   5.19062941,
          6.79953294,   8.40843647,  10.01734   ]),
 <a list of 10 Patch objects>)


plt.hist(df_combined_bumpy_road_data['Z'])
(array([  3.00000000e+00,   1.30000000e+01,   1.48000000e+02,
          1.01200000e+03,   3.14200000e+03,   3.34600000e+03,
          1.29400000e+03,   2.34000000e+02,   2.00000000e+01,
          2.00000000e+00]),
 array([ -2.681506,  -0.344765,   1.991976,   4.328717,   6.665458,
          9.002199,  11.33894 ,  13.675681,  16.012422,  18.349163,
         20.685904]),
 <a list of 10 Patch objects>)


plt.hist(df_combined_bumpy_road_data['Xg'])
(array([  8.00000000e+00,   3.00000000e+01,   3.33000000e+02,
          2.71600000e+03,   4.96200000e+03,   1.03300000e+03,
          1.17000000e+02,   1.00000000e+01,   2.00000000e+00,
          3.00000000e+00]),
 array([-0.7926413 , -0.60663807, -0.42063484, -0.23463161, -0.04862838,
         0.13737485,  0.32337808,  0.50938131,  0.69538454,  0.88138777,
         1.067391  ]),
 <a list of 10 Patch objects>)


plt.hist(df_combined_bumpy_road_data['Yg'])
(array([  2.00000000e+00,   3.00000000e+00,   1.20000000e+01,
          1.66000000e+02,   2.69400000e+03,   5.72900000e+03,
          5.58000000e+02,   3.80000000e+01,   1.00000000e+01,
          2.00000000e+00]),
 array([-0.6976954 , -0.56361048, -0.42952555, -0.29544063, -0.1613557 ,
        -0.02727077,  0.10681415,  0.24089908,  0.374984  ,  0.50906893,
         0.64315385]),
 <a list of 10 Patch objects>)


plt.hist(df_combined_bumpy_road_data['Zg'])
(array([   26.,   616.,  3896.,  3777.,   700.,   126.,    41.,    20.,
            5.,     7.]),
 array([-0.38938296, -0.26009246, -0.13080197, -0.00151147,  0.12777902,
         0.25706952,  0.38636002,  0.51565051,  0.64494101,  0.7742315 ,
         0.903522  ]),
 <a list of 10 Patch objects>)

Combining the data from the positive and negative classes

result=np.vstack((df_combined_carpet_road_data,df_combined_bumpy_road_data))

result.shape
(20720, 7)

result1=pd.DataFrame(data=result,
          index=pd.RangeIndex((20720)),
          columns=['X','Y','Z', 'Xg', 'Yg','Zg', 'Label' ])
​
#df = pd.DataFrame(a, ['x', 'y', 'u', 'z', 'w'], ['a', 'b', 'c', 'd', 'e', 'f', 'g'])
​
​
​
#result.tail()

result1
X	Y	Z	Xg	Yg	Zg	Label
0	-0.076614	2.164358	8.734048	0.002758	-0.039410	-0.024923	1.0
1	0.076614	0.057461	7.354988	-0.003246	-0.018500	-0.001449	1.0
2	-0.497994	3.160346	9.806650	0.005149	-0.008971	0.017383	1.0
3	-0.306458	1.398214	10.879252	0.001937	-0.005288	0.018309	1.0
4	-0.191536	2.738967	9.040505	-0.018832	-0.003665	0.017506	1.0
5	-0.881066	3.428497	9.768343	-0.041399	0.001449	0.022916	1.0
6	-0.268151	2.087744	9.461885	-0.029740	0.002391	0.009913	1.0
7	-0.804452	4.386178	10.572795	-0.017314	0.007435	0.000262	1.0
8	-0.957681	2.011129	8.274361	-0.020298	0.026878	-0.006824	1.0
9	-0.268151	2.853889	8.006210	0.012793	0.048991	-0.016773	1.0
10	-0.421380	3.313575	9.576807	0.019059	0.054018	-0.021956	1.0
11	-0.574608	2.011129	9.117120	-0.025168	0.049306	-0.014748	1.0
12	-0.306458	2.279280	9.078813	-0.027873	0.054175	-0.017174	1.0
13	-0.114922	0.670377	9.538500	-0.013509	0.058556	-0.024190	1.0
14	-0.229843	2.279280	9.423578	-0.034383	0.059202	-0.022864	1.0
15	0.153229	2.432509	9.615114	-0.016074	0.062849	-0.033441	1.0
16	-0.344765	4.616021	10.036493	0.003229	0.043825	-0.043057	1.0
17	-0.421380	1.857901	10.534488	0.013439	0.024749	-0.071611	1.0
18	-0.268151	2.624045	8.427590	0.033650	0.014085	-0.107914	1.0
19	-0.229843	2.279280	10.687716	0.023963	0.008727	-0.104982	1.0
20	-0.306458	2.240973	9.500193	0.018378	0.004939	-0.085207	1.0
21	-0.153229	2.470816	8.274361	0.019722	-0.009896	-0.057020	1.0
22	-0.344765	2.624045	9.232041	0.007330	-0.035256	-0.011083	1.0
23	0.000000	2.355895	8.619126	0.001571	-0.057945	0.035709	1.0
24	-0.268151	2.087744	8.848969	0.012182	-0.080948	0.076672	1.0
25	-0.459687	1.053449	9.346964	-0.009372	-0.088855	0.087144	1.0
26	-0.191536	1.666364	8.810662	-0.011345	-0.085032	0.076690	1.0
27	-0.268151	2.624045	9.270349	-0.047822	-0.074299	0.075276	1.0
28	0.038307	2.279280	9.117120	-0.022270	-0.069290	0.061383	1.0
29	-0.881066	2.087744	7.584831	0.025552	-0.059655	0.044820	1.0
...	...	...	...	...	...	...	...
20690	0.651223	1.819593	10.381259	-0.022078	0.056636	-0.015341	0.0
20691	0.881066	2.432509	9.500193	0.002234	0.038153	-0.007784	0.0
20692	-0.612916	1.244985	12.334928	-0.058050	-0.019234	-0.014661	0.0
20693	-0.038307	2.126051	10.840945	-0.080844	-0.017209	-0.018186	0.0
20694	-0.153229	2.317587	8.887277	-0.045117	0.002705	-0.007418	0.0
20695	-0.038307	2.126051	12.143391	0.001047	0.009442	-0.003316	0.0
20696	-1.187524	-0.938527	12.679692	0.017034	0.008046	0.003473	0.0
20697	-1.302446	1.513135	10.879252	-0.011659	-0.012374	0.000105	0.0
20698	0.038307	0.785298	8.427590	-0.021433	-0.054664	-0.026704	0.0
20699	0.000000	-1.053449	8.389283	-0.055467	-0.081472	-0.050824	0.0
20700	-0.114922	2.738967	10.266336	-0.148144	-0.069255	-0.061575	0.0
20701	-0.995988	1.244985	7.469909	-0.152228	-0.054315	-0.051941	0.0
20702	-0.383072	2.049437	9.308656	-0.136066	-0.006039	-0.048555	0.0
20703	-0.114922	4.386178	11.262324	-0.089658	0.036966	-0.051662	0.0
20704	0.038307	3.466804	9.461885	0.030247	0.026599	-0.039794	0.0
20705	0.344765	3.543418	7.699753	0.154566	0.003072	-0.034924	0.0
20706	0.536301	0.057461	9.461885	0.077911	-0.008238	-0.068836	0.0
20707	-0.153229	1.398214	10.266336	0.002862	-0.009128	-0.086638	0.0
20708	0.153229	2.240973	9.576807	-0.013910	-0.009215	-0.094527	0.0
20709	-0.421380	1.244985	8.734048	-0.080948	-0.012095	-0.108176	0.0
20710	-0.344765	4.539406	7.469909	-0.030247	-0.011851	-0.103655	0.0
20711	0.076614	2.087744	8.657433	-0.026040	-0.010140	-0.096936	0.0
20712	-0.306458	2.738967	6.971915	0.050684	-0.016336	-0.083706	0.0
20713	0.000000	1.283292	6.818687	0.121091	-0.034784	-0.057229	0.0
20714	-0.995988	1.015142	8.006210	0.130778	-0.063181	-0.039689	0.0
20715	-0.038307	0.517148	15.284584	0.059132	-0.062064	-0.034819	0.0
20716	-0.421380	2.470816	9.959879	0.043092	-0.035989	-0.031940	0.0
20717	0.191536	1.091756	10.036493	0.010018	-0.029863	-0.044104	0.0
20718	-0.076614	2.738967	9.346964	-0.011240	-0.034558	-0.061872	0.0
20719	0.153229	0.823605	5.018247	0.019984	-0.028798	-0.054716	0.0
20720 rows × 7 columns

Slicing the data columns

X = result1.loc[:, ['X', 'Y', 'Z','Xg','Yg','Zg']].values
Starting the feature Engineering
Step-1 : Scaling the data

def scaleData(data):
    # normalize features
    scaler = MinMaxScaler(feature_range=(0, 1))
    return scaler.fit_transform(data)
​

normal_data=scaleData(X)

normal_data
array([[ 0.31542057,  0.51190476,  0.48852457,  0.4276264 ,  0.49094696,
         0.28189207],
       [ 0.32476636,  0.38095237,  0.42950817,  0.42439854,  0.50654084,
         0.30004861],
       [ 0.28971964,  0.57380952,  0.53442619,  0.42891192,  0.5136479 ,
         0.31461434],
       ..., 
       [ 0.33177571,  0.44523809,  0.54426224,  0.43152987,  0.49806704,
         0.26705636],
       [ 0.31542057,  0.54761904,  0.5147541 ,  0.42010097,  0.49456558,
         0.25331408],
       [ 0.32943926,  0.42857142,  0.32950818,  0.43688775,  0.49886105,
         0.25884879]])

normalized_train_data=pd.DataFrame(data=normal_data,
          index=pd.RangeIndex((20720)),
          columns=['X', 'Y', 'Z', 'Xg', 'Yg', 'Zg'])
​

normalized_train_data
X	Y	Z	Xg	Yg	Zg
0	0.315421	0.511905	0.488525	0.427626	0.490947	0.281892
1	0.324766	0.380952	0.429508	0.424399	0.506541	0.300049
2	0.289720	0.573810	0.534426	0.428912	0.513648	0.314614
3	0.301402	0.464286	0.580328	0.427185	0.516394	0.315330
4	0.308411	0.547619	0.501639	0.416019	0.517605	0.314709
5	0.266355	0.590476	0.532787	0.403887	0.521419	0.318894
6	0.303738	0.507143	0.519672	0.410155	0.522122	0.308837
7	0.271028	0.650000	0.567213	0.416836	0.525884	0.301372
8	0.261682	0.502381	0.468852	0.415231	0.540384	0.295891
9	0.303738	0.554762	0.457377	0.433022	0.556876	0.288196
10	0.294393	0.583333	0.524590	0.436390	0.560625	0.284187
11	0.285047	0.502381	0.504918	0.412613	0.557110	0.289762
12	0.301402	0.519048	0.503279	0.411159	0.560742	0.287886
13	0.313084	0.419048	0.522951	0.418881	0.564009	0.282459
14	0.306075	0.519048	0.518033	0.407659	0.564491	0.283485
15	0.329439	0.528571	0.526229	0.417502	0.567211	0.275304
16	0.299065	0.664286	0.544262	0.427880	0.553023	0.267866
17	0.294393	0.492857	0.565574	0.433369	0.538796	0.245781
18	0.303738	0.540476	0.475410	0.444235	0.530843	0.217703
19	0.306075	0.519048	0.572131	0.439027	0.526847	0.219971
20	0.301402	0.516667	0.521311	0.436024	0.524022	0.235266
21	0.310748	0.530952	0.468852	0.436747	0.512958	0.257067
22	0.299065	0.540476	0.509836	0.430085	0.494045	0.292597
23	0.320093	0.523810	0.483607	0.426988	0.477123	0.328789
24	0.303738	0.507143	0.493443	0.432693	0.459967	0.360471
25	0.292056	0.442857	0.514754	0.421105	0.454071	0.368571
26	0.308411	0.480952	0.491803	0.420045	0.456922	0.360485
27	0.303738	0.540476	0.511475	0.400434	0.464927	0.359391
28	0.322430	0.519048	0.504918	0.414171	0.468663	0.348646
29	0.266355	0.507143	0.439344	0.439881	0.475848	0.335835
...	...	...	...	...	...	...
20690	0.359813	0.490476	0.559016	0.414274	0.562577	0.289303
20691	0.373832	0.528571	0.521311	0.427345	0.548793	0.295148
20692	0.282710	0.454762	0.642623	0.394935	0.505994	0.289830
20693	0.317757	0.509524	0.578688	0.382680	0.507504	0.287103
20694	0.310748	0.521429	0.495082	0.401888	0.522356	0.295432
20695	0.317757	0.509524	0.634426	0.426707	0.527380	0.298604
20696	0.247664	0.319048	0.657377	0.435302	0.526339	0.303855
20697	0.240654	0.471429	0.580328	0.419876	0.511110	0.301250
20698	0.322430	0.426190	0.475410	0.414621	0.479570	0.280515
20699	0.320093	0.311905	0.473770	0.396324	0.459577	0.261859
20700	0.313084	0.547619	0.554098	0.346498	0.468689	0.253544
20701	0.259346	0.454762	0.434426	0.344302	0.479831	0.260995
20702	0.296729	0.504762	0.513115	0.352991	0.515835	0.263614
20703	0.313084	0.650000	0.596721	0.377942	0.547908	0.261211
20704	0.322430	0.592857	0.519672	0.442405	0.540176	0.270391
20705	0.341122	0.597619	0.444262	0.509243	0.522629	0.274157
20706	0.352804	0.380952	0.519672	0.468031	0.514195	0.247928
20707	0.310748	0.464286	0.554098	0.427683	0.513531	0.234159
20708	0.329439	0.516667	0.524590	0.418665	0.513466	0.228057
20709	0.294393	0.454762	0.488525	0.382624	0.511318	0.217500
20710	0.299065	0.659524	0.434426	0.409883	0.511500	0.220997
20711	0.324766	0.507143	0.485246	0.412144	0.512776	0.226194
20712	0.301402	0.547619	0.413115	0.453393	0.508155	0.236426
20713	0.320093	0.457143	0.406557	0.491245	0.494396	0.256905
20714	0.259346	0.440476	0.457377	0.496453	0.473218	0.270472
20715	0.317757	0.409524	0.768852	0.457935	0.474051	0.274238
20716	0.294393	0.530952	0.540984	0.449311	0.493498	0.276465
20717	0.331776	0.445238	0.544262	0.431530	0.498067	0.267056
20718	0.315421	0.547619	0.514754	0.420101	0.494566	0.253314
20719	0.329439	0.428571	0.329508	0.436888	0.498861	0.258849
20720 rows × 6 columns

Feature Preprocessing Part-2
In the second step, we will apply a low pass filter and pass our data. This will remove any unnecessary noise and will enhance
the predictive power of our classifier

import matplotlib.pyplot as plt
x = np.arange(1, 20721, 1)  # x axis
plt.plot(x,normalized_train_data['X'] , linestyle="-", c="b")
#plt.title('Estimate vs. iteration step')
plt.xlabel('Number of samples')
plt.ylabel('Value of the Sample- X ')
​
Text(0,0.5,'Value of the Sample- X ')


from scipy.signal import lfilter
​
n = 50  # the larger n is, the smoother curve will be
b = [1.0 / n] * n
a = 1
yy = lfilter(b,a,(normalized_train_data['X']))
plt.plot(x, yy, linewidth=2, linestyle="-", c="g")  # smooth by filter
plt.xlabel('Number of samples')
plt.title('Filtered Signal')
Text(0.5,1,'Filtered Signal')


plt.plot(x,normalized_train_data['Y'] , linestyle="-", c="b")
plt.xlabel('Number of samples')
plt.ylabel('Sample Value')
plt.title('Signal Sample- Y')
Text(0.5,1,'Signal Sample- Y')


yy1 = lfilter(b,a,(normalized_train_data['Y']))
plt.plot(x, yy, linewidth=2, linestyle="-", c="g")  # smooth by filter
plt.xlabel('Number of samples')
plt.xlabel('Sample Value')
plt.title('Filtered Signal- Y')
Text(0.5,1,'Filtered Signal- Y')


plt.plot(x,normalized_train_data['Z'] , linestyle="-", c="b")
plt.xlabel('Number of samples')
plt.ylabel('Sample Value')
plt.title('Signal Sample- Z')
Text(0.5,1,'Signal Sample- Z')


yy2 = lfilter(b,a,(normalized_train_data['Z']))
plt.plot(x, yy, linewidth=2, linestyle="-", c="g")  # smooth by filter
plt.xlabel('Number of samples')
plt.xlabel('Sample Value')
plt.title('Filtered Signal ')
Text(0.5,1,'Filtered Signal ')


plt.plot(x,normalized_train_data['Xg'] , linestyle="-", c="b")
plt.xlabel('Number of samples')
plt.ylabel('Sample Value')
plt.title('Signal Sample- Xg')
Text(0.5,1,'Signal Sample- Xg')


yy3 = lfilter(b,a,(normalized_train_data['Xg']))
plt.plot(x, yy, linewidth=2, linestyle="-", c="g")  # smooth by filter
plt.xlabel('Number of samples')
plt.xlabel('Sample Value')
plt.title('Filtered Signal ')
Text(0.5,1,'Filtered Signal ')


plt.plot(x,normalized_train_data['Yg'] , linestyle="-", c="b")
plt.xlabel('Number of samples')
plt.ylabel('Sample Value')
plt.title('Signal Sample- Xg')
Text(0.5,1,'Signal Sample- Xg')


yy4 = lfilter(b,a,(normalized_train_data['Yg']))
plt.plot(x, yy, linewidth=2, linestyle="-", c="g")  # smooth by filter
plt.xlabel('Number of samples')
plt.xlabel('Sample Value')
plt.title('Filtered Signal ')
Text(0.5,1,'Filtered Signal ')


plt.plot(x,normalized_train_data['Zg'] , linestyle="-", c="b")
plt.xlabel('Number of samples')
plt.ylabel('Sample Value')
plt.title('Signal Sample- Zg')
Text(0.5,1,'Signal Sample- Zg')


yy5 = lfilter(b,a,(normalized_train_data['Zg']))
plt.plot(x, yy, linewidth=2, linestyle="-", c="g")  # smooth by filter
plt.xlabel('Number of samples')
plt.xlabel('Sample Value')
plt.title('Filtered Signal ')
Text(0.5,1,'Filtered Signal ')


filtered_train_data_X=pd.DataFrame(data=yy,
          index=pd.RangeIndex((20720)),
          columns=['X'])
​

filtered_train_data_Y=pd.DataFrame(data=yy1,
          index=pd.RangeIndex((20720)),
          columns=['Y'])
​

filtered_train_data_Z=pd.DataFrame(data=yy2,
          index=pd.RangeIndex((20720)),
          columns=['Z'])
​

filtered_train_data_Xg=pd.DataFrame(data=yy3,
          index=pd.RangeIndex((20720)),
          columns=['Xg'])
​

filtered_train_data_Yg=pd.DataFrame(data=yy4,
          index=pd.RangeIndex((20720)),
          columns=['Yg'])
​

filtered_train_data_Zg=pd.DataFrame(data=yy5,
          index=pd.RangeIndex((20720)),
          columns=['Zg'])
​

filtered_train_data_X.reset_index(drop=True, inplace=True)
filtered_train_data_Y.reset_index(drop=True, inplace=True)
filtered_train_data_Z.reset_index(drop=True, inplace=True)
filtered_train_data_Xg.reset_index(drop=True, inplace=True)
filtered_train_data_Yg.reset_index(drop=True, inplace=True)
filtered_train_data_Zg.reset_index(drop=True, inplace=True)
​
df = pd.concat([filtered_train_data_X, filtered_train_data_Y,filtered_train_data_Z,filtered_train_data_Xg,filtered_train_data_Yg,filtered_train_data_Zg], axis=1)
Slicing the label values

y = result1['Label'].values

df
X	Y	Z	Xg	Yg	Zg
0	0.006308	0.010238	0.009770	0.008553	0.009819	0.005638
1	0.012804	0.017857	0.018361	0.017040	0.019950	0.011639
2	0.018598	0.029333	0.029049	0.025619	0.030223	0.017931
3	0.024626	0.038619	0.040656	0.034162	0.040551	0.024238
4	0.030794	0.049571	0.050689	0.042483	0.050903	0.030532
5	0.036121	0.061381	0.061344	0.050561	0.061331	0.036910
6	0.042196	0.071524	0.071738	0.058764	0.071774	0.043086
7	0.047617	0.084524	0.083082	0.067100	0.082291	0.049114
8	0.052850	0.094571	0.092459	0.075405	0.093099	0.055032
9	0.058925	0.105667	0.101607	0.084065	0.104236	0.060796
10	0.064813	0.117333	0.112098	0.092793	0.115449	0.066479
11	0.070514	0.127381	0.122197	0.101045	0.126591	0.072275
12	0.076542	0.137762	0.132262	0.109269	0.137806	0.078032
13	0.082804	0.146143	0.142721	0.117646	0.149086	0.083682
14	0.088925	0.156524	0.153082	0.125799	0.160376	0.089351
15	0.095514	0.167095	0.163607	0.134149	0.171720	0.094857
16	0.101495	0.180381	0.174492	0.142707	0.182781	0.100215
17	0.107383	0.190238	0.185803	0.151374	0.193557	0.105130
18	0.113458	0.201048	0.195311	0.160259	0.204173	0.109484
19	0.119579	0.211429	0.206754	0.169040	0.214710	0.113884
20	0.125607	0.221762	0.217180	0.177760	0.225191	0.118589
21	0.131822	0.232381	0.226557	0.186495	0.235450	0.123730
22	0.137804	0.243190	0.236754	0.195097	0.245331	0.129582
23	0.144206	0.253667	0.246426	0.203637	0.254873	0.136158
24	0.150280	0.263810	0.256295	0.212290	0.264073	0.143368
25	0.156122	0.272667	0.266590	0.220713	0.273154	0.150739
26	0.162290	0.282286	0.276426	0.229113	0.282292	0.157949
27	0.168364	0.293095	0.286656	0.237122	0.291591	0.165136
28	0.174813	0.303476	0.296754	0.245406	0.300964	0.172109
29	0.180140	0.313619	0.305541	0.254203	0.310481	0.178826
...	...	...	...	...	...	...
20690	0.307056	0.536952	0.503410	0.413310	0.512334	0.281021
20691	0.307570	0.537857	0.504000	0.415559	0.514607	0.282143
20692	0.307196	0.534619	0.510492	0.415514	0.515999	0.282960
20693	0.306028	0.533667	0.512229	0.414376	0.517271	0.283530
20694	0.307757	0.532714	0.509607	0.413172	0.518441	0.283903
20695	0.308271	0.531524	0.509869	0.413085	0.519546	0.284638
20696	0.307804	0.526095	0.511541	0.414206	0.520542	0.285920
20697	0.307430	0.521714	0.513803	0.416129	0.521114	0.287500
20698	0.307336	0.518667	0.514000	0.418703	0.520946	0.288549
20699	0.308879	0.512762	0.514852	0.420172	0.520142	0.288878
20700	0.310421	0.513048	0.515016	0.420354	0.518978	0.288448
20701	0.309533	0.509000	0.517902	0.419694	0.517427	0.287625
20702	0.309720	0.512810	0.518131	0.418700	0.516450	0.286752
20703	0.310794	0.517952	0.519574	0.417515	0.515331	0.285624
20704	0.310514	0.522667	0.520361	0.416910	0.513896	0.284529
20705	0.310701	0.525667	0.521934	0.417134	0.511919	0.283261
20706	0.311495	0.524571	0.521410	0.416466	0.509673	0.281487
20707	0.310981	0.524143	0.522951	0.415278	0.507671	0.279477
20708	0.310561	0.524762	0.520852	0.414755	0.506248	0.277324
20709	0.310421	0.523381	0.521770	0.414359	0.505002	0.275161
20710	0.309813	0.527524	0.520852	0.414642	0.503809	0.273094
20711	0.309907	0.526095	0.521770	0.414726	0.502990	0.271363
20712	0.309626	0.528714	0.518820	0.415621	0.502404	0.270261
20713	0.309206	0.527952	0.517049	0.417393	0.501992	0.269536
20714	0.308692	0.527238	0.516361	0.419905	0.501749	0.269237
20715	0.308925	0.525429	0.521902	0.423582	0.501745	0.269507
20716	0.308131	0.523524	0.524721	0.427135	0.502921	0.269791
20717	0.307897	0.521381	0.527180	0.431072	0.504953	0.269953
20718	0.308785	0.521190	0.527115	0.434003	0.506786	0.269596
20719	0.308084	0.518095	0.523049	0.436205	0.508782	0.269164
20720 rows × 6 columns


#data_healthy_fft = np.fft.fft(data_healthy)
#df= np.fft.fft(df)
​

df
X	Y	Z	Xg	Yg	Zg
0	0.006308	0.010238	0.009770	0.008553	0.009819	0.005638
1	0.012804	0.017857	0.018361	0.017040	0.019950	0.011639
2	0.018598	0.029333	0.029049	0.025619	0.030223	0.017931
3	0.024626	0.038619	0.040656	0.034162	0.040551	0.024238
4	0.030794	0.049571	0.050689	0.042483	0.050903	0.030532
5	0.036121	0.061381	0.061344	0.050561	0.061331	0.036910
6	0.042196	0.071524	0.071738	0.058764	0.071774	0.043086
7	0.047617	0.084524	0.083082	0.067100	0.082291	0.049114
8	0.052850	0.094571	0.092459	0.075405	0.093099	0.055032
9	0.058925	0.105667	0.101607	0.084065	0.104236	0.060796
10	0.064813	0.117333	0.112098	0.092793	0.115449	0.066479
11	0.070514	0.127381	0.122197	0.101045	0.126591	0.072275
12	0.076542	0.137762	0.132262	0.109269	0.137806	0.078032
13	0.082804	0.146143	0.142721	0.117646	0.149086	0.083682
14	0.088925	0.156524	0.153082	0.125799	0.160376	0.089351
15	0.095514	0.167095	0.163607	0.134149	0.171720	0.094857
16	0.101495	0.180381	0.174492	0.142707	0.182781	0.100215
17	0.107383	0.190238	0.185803	0.151374	0.193557	0.105130
18	0.113458	0.201048	0.195311	0.160259	0.204173	0.109484
19	0.119579	0.211429	0.206754	0.169040	0.214710	0.113884
20	0.125607	0.221762	0.217180	0.177760	0.225191	0.118589
21	0.131822	0.232381	0.226557	0.186495	0.235450	0.123730
22	0.137804	0.243190	0.236754	0.195097	0.245331	0.129582
23	0.144206	0.253667	0.246426	0.203637	0.254873	0.136158
24	0.150280	0.263810	0.256295	0.212290	0.264073	0.143368
25	0.156122	0.272667	0.266590	0.220713	0.273154	0.150739
26	0.162290	0.282286	0.276426	0.229113	0.282292	0.157949
27	0.168364	0.293095	0.286656	0.237122	0.291591	0.165136
28	0.174813	0.303476	0.296754	0.245406	0.300964	0.172109
29	0.180140	0.313619	0.305541	0.254203	0.310481	0.178826
...	...	...	...	...	...	...
20690	0.307056	0.536952	0.503410	0.413310	0.512334	0.281021
20691	0.307570	0.537857	0.504000	0.415559	0.514607	0.282143
20692	0.307196	0.534619	0.510492	0.415514	0.515999	0.282960
20693	0.306028	0.533667	0.512229	0.414376	0.517271	0.283530
20694	0.307757	0.532714	0.509607	0.413172	0.518441	0.283903
20695	0.308271	0.531524	0.509869	0.413085	0.519546	0.284638
20696	0.307804	0.526095	0.511541	0.414206	0.520542	0.285920
20697	0.307430	0.521714	0.513803	0.416129	0.521114	0.287500
20698	0.307336	0.518667	0.514000	0.418703	0.520946	0.288549
20699	0.308879	0.512762	0.514852	0.420172	0.520142	0.288878
20700	0.310421	0.513048	0.515016	0.420354	0.518978	0.288448
20701	0.309533	0.509000	0.517902	0.419694	0.517427	0.287625
20702	0.309720	0.512810	0.518131	0.418700	0.516450	0.286752
20703	0.310794	0.517952	0.519574	0.417515	0.515331	0.285624
20704	0.310514	0.522667	0.520361	0.416910	0.513896	0.284529
20705	0.310701	0.525667	0.521934	0.417134	0.511919	0.283261
20706	0.311495	0.524571	0.521410	0.416466	0.509673	0.281487
20707	0.310981	0.524143	0.522951	0.415278	0.507671	0.279477
20708	0.310561	0.524762	0.520852	0.414755	0.506248	0.277324
20709	0.310421	0.523381	0.521770	0.414359	0.505002	0.275161
20710	0.309813	0.527524	0.520852	0.414642	0.503809	0.273094
20711	0.309907	0.526095	0.521770	0.414726	0.502990	0.271363
20712	0.309626	0.528714	0.518820	0.415621	0.502404	0.270261
20713	0.309206	0.527952	0.517049	0.417393	0.501992	0.269536
20714	0.308692	0.527238	0.516361	0.419905	0.501749	0.269237
20715	0.308925	0.525429	0.521902	0.423582	0.501745	0.269507
20716	0.308131	0.523524	0.524721	0.427135	0.502921	0.269791
20717	0.307897	0.521381	0.527180	0.431072	0.504953	0.269953
20718	0.308785	0.521190	0.527115	0.434003	0.506786	0.269596
20719	0.308084	0.518095	0.523049	0.436205	0.508782	0.269164
20720 rows × 6 columns


df=pd.DataFrame(data=df,
          index=pd.RangeIndex((20720)),
          columns=['X','Y','Z','Xg','Yg','Zg'])
​

#df

from sklearn.model_selection import train_test_split
Splitting the test-train dataset...
By convention, we take 30% of the data as the testset and 70% as the training data set

​
X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)

y_train
array([ 1.,  0.,  0., ...,  0.,  0.,  1.])

y_test
array([ 1.,  1.,  0., ...,  0.,  0.,  0.])
Model Selection...
For this task, we will compare at least 1 neural network based model and one Decision Tree based Classifier
For a neural network appraoch, we use a simple Feed-Forward Network
For a Decision Tree based Classifier, we use the Gradient Boosting Classifier ( or Random Forest Classifier)...
And we will compare the model Performance

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier
​
from sklearn.metrics import classification_report
from sklearn.model_selection import learning_curve
from sklearn.linear_model import LinearRegression
Decision Tree Based Classifier... We use Random Forest Classifier

#clf=LinearRegression()
clf = RandomForestClassifier(n_estimators=10000, max_depth=12, random_state=42)
#from sklearn import svm

clf.fit(X_train,y_train)
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=12, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10000, n_jobs=1,
            oob_score=False, random_state=42, verbose=0, warm_start=False)

clf.score(X_test, y_test)
0.87379343629343631

predictions = clf.predict(X_test)

print(classification_report(y_test, predictions))
             precision    recall  f1-score   support

        0.0       0.95      0.76      0.84      1841
        1.0       0.83      0.97      0.89      2303

avg / total       0.88      0.87      0.87      4144


from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, predictions)
array([[1394,  447],
       [  76, 2227]])

tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()

(tn, fp, fn, tp)
(1394, 447, 76, 2227)

 from sklearn import metrics
fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions, pos_label=2)
/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/metrics/ranking.py:571: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  UndefinedMetricWarning)
Feed Forward Neural Network Approach

from keras.models import Sequential
model = Sequential()
from keras.layers import Dense
​
model.add(Dense(units=1000, activation='relu', input_dim=6))
model.add(Dense(units=400, activation='relu'))
model.add(Dense(units=300, activation='relu'))
model.add(Dense(units=200, activation='relu'))
model.add(Dense(units=100, activation='relu'))
​
​
​
model.add(Dense(units=1, activation='softmax'))

model.compile(loss='mse',
              optimizer='sgd',
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=5, batch_size=320)
Epoch 1/5
16576/16576 [==============================] - 5s 304us/step - loss: 0.4448 - acc: 0.5552
Epoch 2/5
16576/16576 [==============================] - 5s 307us/step - loss: 0.4448 - acc: 0.5552
Epoch 3/5
16576/16576 [==============================] - 5s 296us/step - loss: 0.4448 - acc: 0.5552
Epoch 4/5
16576/16576 [==============================] - 6s 374us/step - loss: 0.4448 - acc: 0.5552
Epoch 5/5
16576/16576 [==============================] - 5s 295us/step - loss: 0.4448 - acc: 0.5552
<keras.callbacks.History at 0x7ff8944c7358>

predictions1 = model.predict(X_test)

print(classification_report(y_test, predictions1))
             precision    recall  f1-score   support

        0.0       0.00      0.00      0.00      1841
        1.0       0.56      1.00      0.71      2303

avg / total       0.31      0.56      0.40      4144

/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
Comparing the Performance parameters of Neural Network Model and Decision tree based model

​
